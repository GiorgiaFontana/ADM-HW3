{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "RZS5UpX0x7-H",
   "metadata": {
    "id": "RZS5UpX0x7-H"
   },
   "source": [
    "##  ADM - Homework 3 - What is the best anime in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb79ad-b719-42b5-be26-6ee4c7559b25",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6caabf0-cef5-4ffd-aca0-109d17d58522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from itertools import islice\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import heapq\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IfWAAiT7yIej",
   "metadata": {
    "id": "IfWAAiT7yIej"
   },
   "source": [
    "# 1 - Data collection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qY689yAezlQ9",
   "metadata": {
    "id": "qY689yAezlQ9"
   },
   "source": [
    "## 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "na6tiRoN0MHQ",
   "metadata": {
    "id": "na6tiRoN0MHQ"
   },
   "source": [
    "We collect the anime's url contained in all the pages of the \"Top Anime Series\" list (383 pages).\n",
    "We retrieve them using the html of the page and discovering where the urls are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PDKI1z0B1Fjw",
   "metadata": {
    "id": "PDKI1z0B1Fjw"
   },
   "outputs": [],
   "source": [
    "# get urls from first page\n",
    "html = urlopen(\"https://myanimelist.net/topanime.php\")  # open link\n",
    "soup = BeautifulSoup(html.read(), features=\"html5lib\")  # read the html\n",
    "# encoding because there are chars that are not readable so i'll ignore them\n",
    "soup.prettify().encode(errors='ignore')\n",
    "links = []\n",
    "# search this class in h3 element (we saw that each url is contained in this class)\n",
    "for link in soup.find_all(\"h3\", {\"class\": \"hoverinfo_trigger fl-l fs14 fw-b anime_ranking_h3\"}):\n",
    "    a = link.find(\"a\")  # find \"a\" element\n",
    "    # get the url in href and append to the list\n",
    "    links.append(a.get(\"href\").encode('utf-8').decode('ascii', 'ignore'))\n",
    "textfile = open(\"anime_url.txt\", \"w\")  # open the file\n",
    "for element in links:\n",
    "    textfile.write(element + \"\\n\")  # write each url\n",
    "\n",
    "# do the same for the next pages\n",
    "lim = 50\n",
    "for i in range(382):\n",
    "    links = []\n",
    "    # we saw that the following pages follow this pattern\n",
    "    html = urlopen(\"https://myanimelist.net/topanime.php?limit=\"+str(lim))\n",
    "    soup = BeautifulSoup(html.read(), features=\"html5lib\")\n",
    "    for link in soup.find_all(\"h3\", {\"class\": \"hoverinfo_trigger fl-l fs14 fw-b anime_ranking_h3\"}):\n",
    "        a = link.find(\"a\")\n",
    "        links.append(a.get(\"href\").encode('utf-8').decode('ascii', 'ignore'))\n",
    "    for element in links:\n",
    "        textfile.write(element + \"\\n\")\n",
    "    lim += 50\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nLjbvDwS1jjC",
   "metadata": {
    "id": "nLjbvDwS1jjC"
   },
   "source": [
    "After executing this code we have the file \"anime_url.txt\" that contains all the anime's url, one for each line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GrpbyBm62IzX",
   "metadata": {
    "id": "GrpbyBm62IzX"
   },
   "source": [
    "## 1.2 Crawl animes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QaUx6ds62OhH",
   "metadata": {
    "id": "QaUx6ds62OhH"
   },
   "source": [
    "Using the urls in the file previously created, we download every anime's html and then store them in folders, following the organization in pages of the site (50 anime in each folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6f12b-65b4-403e-9600-58ca573a7aa0",
   "metadata": {
    "id": "d8c6f12b-65b4-403e-9600-58ca573a7aa0"
   },
   "outputs": [],
   "source": [
    "ini = 1  # value that corresponds to the number of line where start in \"anime_url.txt\" --> used to do it in parallel\n",
    "for j in range(383):  # number that corresponds to the number of pages to do --> used to do it in parallel\n",
    "    # set the path to use for the folder\n",
    "    path = \"Anime/page_\"+str(math.floor(ini//50)+1)\n",
    "    os.mkdir(path)  # create the folder\n",
    "    with open('anime_url.txt') as f:\n",
    "        # take 50 lines at time of the document (50 anime on each page)\n",
    "        next_n_lines = list(islice(f, ini, ini+50))\n",
    "        for i in range(50):\n",
    "            r = requests.get(next_n_lines[i])  # get the url\n",
    "            # create the file html and write inside the content of the page\n",
    "            with open(path+\"/article_\"+str(ini+i+1)+\".html\", 'wb') as outfile:\n",
    "                outfile.write(r.content)\n",
    "        ini += 50  # update the index of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DtTHIUJY3dKO",
   "metadata": {
    "id": "DtTHIUJY3dKO"
   },
   "source": [
    "After this we will have a structure like this:\n",
    "\n",
    "Anime\n",
    "> /page_1\n",
    "\n",
    "\n",
    "> > article_1.html\n",
    "\n",
    "> > article_2.html\n",
    "\n",
    "> > ...\n",
    "\n",
    "> > article_50.html\n",
    "\n",
    "> /page_2\n",
    "\n",
    "\n",
    "> > article_51.html\n",
    "\n",
    "> > article_52.html\n",
    "\n",
    "> > ...\n",
    "\n",
    "> > article_100.html\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad390f-c6eb-43da-809f-1ef585a10361",
   "metadata": {
    "id": "73ad390f-c6eb-43da-809f-1ef585a10361"
   },
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yuhf7w1m6n2o",
   "metadata": {
    "id": "yuhf7w1m6n2o"
   },
   "source": [
    "Now, we retrieve all the information we need, related to the anime, from each html.\n",
    "We do this, again, using the html structure and the library BeautifulSoup and then we store all the information we got, in tsv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cabeb-ef65-4f06-a502-7b066935556d",
   "metadata": {
    "id": "756cabeb-ef65-4f06-a502-7b066935556d"
   },
   "outputs": [],
   "source": [
    "ini = 1 # initial value \n",
    "for i in range(383): # n of pages to parse\n",
    "    page = \"Anime/page_\"+str(ini+i)\n",
    "    first = ((ini+i-1)*50)+1 # every time get the number of the first article of the page\n",
    "    for j in range(50): # 50 anime for page\n",
    "        n_article = str(first+j)\n",
    "        path = page + '/' + 'article_' + n_article + '.html'\n",
    "        \n",
    "        # Open anime page and extract html content\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        \n",
    "        # Check for 404 error --> if we have it, we skip that html\n",
    "        error = soup.find(\"div\", attrs={\"class\":\"error404\"})\n",
    "        if error:\n",
    "            print('article_' + n_article + 'was skipped due to 404 error')\n",
    "            continue\n",
    "        \n",
    "        # Title\n",
    "        animeTitle = soup.find(\"h1\", attrs ={\"class\":\"title-name h1_bold_none\"}).get_text(strip=True)\n",
    "        \n",
    "        ## Table information\n",
    "        border = soup.find(\"td\", attrs ={\"class\":\"borderClass\"})\n",
    "        rawtype = border.find(\"h2\", text = \"Information\").next_sibling.next_sibling.get_text(strip = True)\n",
    "        # Anime Type\n",
    "        animeType = rawtype.split(':')[-1]\n",
    "        # Anime number episode\n",
    "        rawepisode = border.find(\"span\", attrs ={\"class\":\"dark_text\"}, text = \"Episodes:\").next_sibling.get_text(strip=True)\n",
    "        try:\n",
    "            animeNumEpisode = int(rawepisode)\n",
    "        except:\n",
    "            animeNumEpisode = ''\n",
    "        # Release and End date\n",
    "        aired = border.find(\"span\", attrs ={\"class\":\"dark_text\"}, text = \"Aired:\").next_sibling.get_text(strip=True)\n",
    "        start, end = aired.split('to')[0], aired.split('to')[-1]\n",
    "        if len(start.strip()) == 4:\n",
    "            releaseDate = datetime.strptime(start.strip(), '%Y').year\n",
    "        else:\n",
    "            try:\n",
    "                releaseDate = datetime.strptime(start.strip(), '%b %d, %Y').date()\n",
    "            except:\n",
    "                releaseDate = ''\n",
    "        if len(end.strip()) == 4:\n",
    "            endDate = datetime.strptime(end.strip(), '%Y').year\n",
    "        else:\n",
    "            try:\n",
    "                endDate = datetime.strptime(end.strip(), '%b %d, %Y').date()\n",
    "            except:\n",
    "                endDate = ''\n",
    "        \n",
    "        ## Upper bar\n",
    "        header = soup.find(\"div\", attrs={\"class\":\"anime-detail-header-stats di-tc va-t\"})\n",
    "        # Anime score\n",
    "        rawscore = header.find(\"div\", attrs={\"class\":\"fl-l score\"}).get_text()\n",
    "        try:\n",
    "            animeScore = float(rawscore)\n",
    "        except:\n",
    "            animeScore = ''\n",
    "        # Members\n",
    "        members = header.find(\"span\", attrs={\"class\":\"numbers members\"}).get_text().split()[1]\n",
    "        rawmembers = members.replace(',', '')\n",
    "        try:\n",
    "            animeNumMembers = int(rawmembers)\n",
    "        except:\n",
    "            animeNumMembers = ''\n",
    "        # Rank\n",
    "        rank = header.find(\"span\", attrs={\"class\":\"numbers ranked\"}).get_text().split()[1]\n",
    "        rawrank = rank.replace('#', '')\n",
    "        try:\n",
    "            animeRank = int(rawrank)\n",
    "        except:\n",
    "            animeRank = ''\n",
    "        # Anime Users\n",
    "        users = header.find(\"div\", attrs={\"class\":\"fl-l score\"})['data-user'].split()[0]\n",
    "        rawusers = users.replace(',','')\n",
    "        try:\n",
    "            animeUsers = int(rawusers)\n",
    "        except:\n",
    "            animeUsers = ''\n",
    "        # Popularity\n",
    "        pop = header.find(\"span\", attrs={\"class\":\"numbers popularity\"}).get_text().split()[1]\n",
    "        rawpopularity = pop.replace('#', '')\n",
    "        try:\n",
    "            animePopularity = int(rawpopularity)\n",
    "        except:\n",
    "            animePopularity = ''\n",
    "        \n",
    "        # Synopsis (description)\n",
    "        animeDescription = soup.find(\"p\", attrs ={\"itemprop\":\"description\"}).get_text(strip=True)\n",
    "        \n",
    "        # Related Anime\n",
    "        table = soup.find(\"table\", attrs={\"class\":\"anime_detail_related_anime\"})\n",
    "        animeRelated = []\n",
    "        if table:\n",
    "            for x in table.find_all(\"td\", attrs={\"class\":\"borderClass\"}):\n",
    "                for y in x.find_all(\"a\"):\n",
    "                    animeRelated.append(y.get_text(strip=True))\n",
    "                    \n",
    "        # Characters, voices and staff\n",
    "        # each case: where we have all, some of them or no-one\n",
    "        tables = soup.find_all(\"div\", attrs={\"class\":\"detail-characters-list clearfix\"})\n",
    "        if len(tables) == 2:\n",
    "            table, table_staff = tables\n",
    "            animeCharacters = []\n",
    "            for x in table.find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"align\":None}):\n",
    "                char = x.get_text(\"/\", strip=True).split('/')[0]\n",
    "                if char:\n",
    "                    animeCharacters.append(char)\n",
    "\n",
    "            animeVoices = []\n",
    "            for x in table.find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"align\":\"right\"}):\n",
    "                voice = x.get_text(\"/\", strip=True).split('/')[0]\n",
    "                if voice:\n",
    "                    animeVoices.append(voice)\n",
    "\n",
    "            animeStaff = []\n",
    "            for x in table_staff.find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"width\":None}):\n",
    "                member = (x.get_text(\"/\", strip=True).split('/'))\n",
    "                animeStaff.append(member)\n",
    "        elif len(tables) == 1:\n",
    "            animeCharacters = ''\n",
    "            animeVoices = ''\n",
    "            animeStaff = []\n",
    "            for x in tables[0].find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"width\":None}):\n",
    "                member = (x.get_text(\"/\", strip=True).split('/'))\n",
    "                animeStaff.append(member)\n",
    "        else:\n",
    "            animeCharacters = ''\n",
    "            animeVoices = ''\n",
    "            animeStaff = ''\n",
    "\n",
    "        # Saving tsv file\n",
    "        tsvname = 'anime_'+ n_article + '.tsv'\n",
    "        path = 'File tsv/'+tsvname\n",
    "        to_write = [animeTitle, animeType, animeNumEpisode, releaseDate, endDate,\n",
    "                               animeNumMembers, animeScore, animeUsers, animeRank, animePopularity,\n",
    "                               animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff]\n",
    "        with open(path, 'wt', encoding = 'utf-8') as out:\n",
    "            tsv_writer = csv.writer(out, delimiter = '\\t')\n",
    "            tsv_writer.writerow([\"animeTitle\", \"animeType\", \"animeNumEpisode\", \"releaseDate\", \"endDate\",\n",
    "                               \"animeNumMembers\", \"animeScore\", \"animeUsers\", \"animeRank\", \"animePopularity\",\n",
    "                               \"animeDescription\", \"animeRelated\", \"animeCharacters\", \"animeVoices\", \"animeStaff\"])\n",
    "            tsv_writer.writerow(to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74vFxJc0ACVl",
   "metadata": {
    "id": "74vFxJc0ACVl"
   },
   "source": [
    "### In the following bunch of script, we put an **example of the execution** of each branch of the code on a single html to show how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_6xfo95rA6it",
   "metadata": {
    "id": "_6xfo95rA6it"
   },
   "source": [
    "\n",
    "\n",
    "*   How a tsv file created looks like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a6537-0d88-4d69-8fec-62f17a14a7b4",
   "metadata": {
    "id": "dd2a6537-0d88-4d69-8fec-62f17a14a7b4",
    "outputId": "8faefc86-6954-42a7-8933-ecc4434c0e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
      "[]\n",
      "['Ashita e Free Kick', 'TV', '52', '1992-04-14', '1993-04-24', '1873', '6.42', '611', '6551', '9603', 'Shun Godai is a young boy who likes soccer. But his grandfather, a successful businessman, want his grandson to follow his path...', '[]', \"['Godai, Shun', 'Mascowitz, Jose', 'Aritaka, Mizuho', 'Henderson, Carl', 'Toto', 'Morita, Mirei', 'Bazettini, Roberto', 'Randou, Shin', 'Green, Abe', 'Becken, Chris']\", \"['Kusao, Takeshi', 'Takagi, Wataru', 'Amano, Yuri', 'Ishino, Ryuuzou', 'Yamaguchi, Kappei', 'Orikasa, Ai', 'Sasaki, Nozomu', 'Koyasu, Takehito', 'Iwanaga, Tetsuya', 'Kosugi, Juurouta']\", '[[\\'Amino, Tetsuro\\', \\'Director\\'], [\\'Itou, Naoyuki\\', \\'Episode Director, Animation Director\\'], [\\'Kato, Takao\\', \\'Episode Director, Storyboard\\'], [\"D\\'Avena, Cristina\", \\'Theme Song Performance\\']]']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with open('anime_6551.tsv') as f:\n",
    "    content = csv.reader(f, delimiter = '\\t')\n",
    "    for line in content:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274fc10-114a-40f1-9313-c4e2694b551b",
   "metadata": {
    "id": "6274fc10-114a-40f1-9313-c4e2694b551b"
   },
   "outputs": [],
   "source": [
    "with open(\"Anime/page_131/article_6519.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d5425-7e6a-49c3-903c-523da83fc373",
   "metadata": {
    "id": "e45d5425-7e6a-49c3-903c-523da83fc373"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rQpCnrthBpLH",
   "metadata": {
    "id": "rQpCnrthBpLH"
   },
   "source": [
    "\n",
    "\n",
    "*   Get the title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d32be2-a17a-47c9-903b-406adb4f0ca9",
   "metadata": {
    "id": "04d32be2-a17a-47c9-903b-406adb4f0ca9",
    "outputId": "6bf803c9-3bc3-4b0c-a99e-128cea6e81ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blue Archive'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title\n",
    "animeTitle = soup.find(\"h1\", attrs ={\"class\":\"title-name h1_bold_none\"}).get_text(strip=True)\n",
    "animeTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8C1W_c4oCVkq",
   "metadata": {
    "id": "8C1W_c4oCVkq"
   },
   "source": [
    "\n",
    "\n",
    "*  Get the info from the table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b6282-8e6b-4cf0-9cb0-fb68e7568dba",
   "metadata": {
    "id": "0e1b6282-8e6b-4cf0-9cb0-fb68e7568dba",
    "outputId": "3448cd03-45b9-464f-dfa9-64346c52ecd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONA NaN 2021-01-25 NaN\n"
     ]
    }
   ],
   "source": [
    "## Table information\n",
    "border = soup.find(\"td\", attrs ={\"class\":\"borderClass\"})\n",
    "rawtype = border.find(\"h2\", text = \"Information\").next_sibling.next_sibling.get_text(strip = True)\n",
    "# Anime Type\n",
    "animeType = rawtype.split(':')[-1]\n",
    "# Anime number episode\n",
    "rawepisode = border.find(\"span\", attrs ={\"class\":\"dark_text\"}, text = \"Episodes:\").next_sibling.get_text(strip=True)\n",
    "try:\n",
    "    animeNumEpisode = int(rawepisode)\n",
    "except:\n",
    "    animeNumEpisode = ''\n",
    "# Release and End date\n",
    "aired = border.find(\"span\", attrs ={\"class\":\"dark_text\"}, text = \"Aired:\").next_sibling.get_text(strip=True)\n",
    "start, end = aired.split('to')[0], aired.split('to')[-1]\n",
    "if len(start.strip()) == 4:\n",
    "  releaseDate = datetime.strptime(start.strip(), '%Y').year\n",
    "else:\n",
    "    try:\n",
    "        releaseDate = datetime.strptime(start.strip(), '%b %d, %Y').date()\n",
    "    except:\n",
    "        releaseDate = ''\n",
    "if len(end.strip()) == 4:\n",
    "    endDate = datetime.strptime(end.strip(), '%Y').year\n",
    "else:\n",
    "    try:\n",
    "        endDate = datetime.strptime(end.strip(), '%b %d, %Y').date()\n",
    "    except:\n",
    "        endDate = ''\n",
    "print(animeType, animeNumEpisode, releaseDate, endDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VYwlVueFCv2n",
   "metadata": {
    "id": "VYwlVueFCv2n"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "*   Get info from the upper bar\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99964ac-1671-45c0-bbe7-2cd971e1a709",
   "metadata": {
    "id": "e99964ac-1671-45c0-bbe7-2cd971e1a709",
    "outputId": "0bab907b-5cd9-4ee7-d97d-4376bf73c42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.45 5698 6420 1788 6649\n"
     ]
    }
   ],
   "source": [
    "# Upper bar\n",
    "header = soup.find(\"div\", attrs={\"class\":\"anime-detail-header-stats di-tc va-t\"})\n",
    "animeScore = float(header.find(\"div\", attrs={\"class\":\"fl-l score\"}).get_text())\n",
    "members = header.find(\"span\", attrs={\"class\":\"numbers members\"}).get_text().split()[1]\n",
    "animeNumMembers = int(members.replace(',', ''))\n",
    "rank = header.find(\"span\", attrs={\"class\":\"numbers ranked\"}).get_text().split()[1]\n",
    "animeRank = int(rank.replace('#', ''))\n",
    "users = header.find(\"div\", attrs={\"class\":\"fl-l score\"})['data-user'].split()[0]\n",
    "animeUsers = int(users.replace(',',''))\n",
    "pop = header.find(\"span\", attrs={\"class\":\"numbers popularity\"}).get_text().split()[1]\n",
    "animePopularity = int(pop.replace('#', ''))\n",
    "print(animeScore, animeNumMembers, animeRank, animeUsers, animePopularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DzCWU2pUC6zk",
   "metadata": {
    "id": "DzCWU2pUC6zk"
   },
   "source": [
    "\n",
    "\n",
    "*   Get the synopsis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368a125-04b6-4836-aede-0469d43d738f",
   "metadata": {
    "id": "0368a125-04b6-4836-aede-0469d43d738f",
    "outputId": "380ff822-3971-4d47-9765-89595cf2ba99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recap featuring fairies with new narration by Watashi.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synopsis\n",
    "animeDescription = soup.find(\"p\", attrs ={\"itemprop\":\"description\"}).get_text(strip=True)\n",
    "animeDescription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O52t7BnKC-fV",
   "metadata": {
    "id": "O52t7BnKC-fV"
   },
   "source": [
    "\n",
    "\n",
    "*   Get the related anime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846ec68-a4c2-434c-bcc9-c0cf061e59c9",
   "metadata": {
    "id": "d846ec68-a4c2-434c-bcc9-c0cf061e59c9",
    "outputId": "249a7eb9-9e02-4d26-a7d9-cd408aa7af1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jinrui wa Suitai Shimashita']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Related\n",
    "table = soup.find(\"table\", attrs={\"class\":\"anime_detail_related_anime\"})\n",
    "animeRelated = []\n",
    "if table:\n",
    "    for x in table.find_all(\"td\", attrs={\"class\":\"borderClass\"}):\n",
    "        for y in x.find_all(\"a\"):\n",
    "            animeRelated.append(y.get_text(strip=True))\n",
    "animeRelated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M1soMFxgDErL",
   "metadata": {
    "id": "M1soMFxgDErL"
   },
   "source": [
    "\n",
    "\n",
    "*   Get characters, voices and staff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5db21c-4d00-4352-9696-b634e07ac47d",
   "metadata": {
    "id": "2e5db21c-4d00-4352-9696-b634e07ac47d",
    "outputId": "4d20c162-5fd3-4a12-a7fc-831671e08fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Watashi', 'Yousei-san', 'Y', 'Joshu-san'] ['Nakahara, Mai'] [['Tanaka, Romeo', 'Original Creator'], ['Tobe, Sunaho', 'Original Character Design']]\n"
     ]
    }
   ],
   "source": [
    "# Characters, voices and staff\n",
    "table, table_staff = soup.find_all(\"div\", attrs={\"class\":\"detail-characters-list clearfix\"})\n",
    "\n",
    "animeCharacters = []\n",
    "for x in table.find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"align\":None}):\n",
    "    char = x.get_text(\"/\", strip=True).split('/')[0]\n",
    "    if char:\n",
    "        animeCharacters.append(char)\n",
    "        \n",
    "animeVoices = []\n",
    "for x in table.find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"align\":\"right\"}):\n",
    "    voice = x.get_text(\"/\", strip=True).split('/')[0]\n",
    "    if voice:\n",
    "        animeVoices.append(voice)\n",
    "        \n",
    "animeStaff = []\n",
    "for x in table_staff.find_all('td', attrs={\"class\":\"borderClass\", \"valign\":\"top\", \"width\":None}):\n",
    "    member = (x.get_text(\"/\", strip=True).split('/'))\n",
    "    animeStaff.append(member)\n",
    "    \n",
    "print(animeCharacters, animeVoices, animeStaff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xBIm7PDKDjUn",
   "metadata": {
    "id": "xBIm7PDKDjUn"
   },
   "source": [
    "# 2 - Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byqEZj9jHppB",
   "metadata": {
    "id": "byqEZj9jHppB"
   },
   "source": [
    "Before creating the search engines, we create a function that computes the pre-process on the text using the nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vszx083UIU2P",
   "metadata": {
    "id": "vszx083UIU2P"
   },
   "source": [
    "\n",
    "\n",
    "*   **Pre_processing_text:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dlaHp2N7HY5Q",
   "metadata": {
    "id": "dlaHp2N7HY5Q"
   },
   "outputs": [],
   "source": [
    "def pre_process_text(text):\n",
    "    # Removing stopwords\n",
    "    # The synopsis is tokenized (divided into words)\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    tokens_without_sw = [\n",
    "        word for word in text_tokens if word not in stopwords.words()]\n",
    "\n",
    "    # join the list of above words to create a sentence without stop words\n",
    "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "    # print(filtered_sentence)\n",
    "\n",
    "    # Removing punctuation\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    text_without_punctuation = tokenizer.tokenize(filtered_sentence)\n",
    "    text_without_punctuation2 = (\" \").join(text_without_punctuation)\n",
    "    # print(text_without_punctuation2)\n",
    "\n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    new_synopsis = []\n",
    "    #new_synopsis = list(map(porter.stem, text_without_punctuation))\n",
    "    for word in text_without_punctuation:\n",
    "        w = porter.stem(word)\n",
    "        new_synopsis.append(w)\n",
    "    return new_synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D_t8O001JEI2",
   "metadata": {
    "id": "D_t8O001JEI2"
   },
   "source": [
    "## 2.1 Conjuntive query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SphMHQ5_J4yC",
   "metadata": {
    "id": "SphMHQ5_J4yC"
   },
   "source": [
    "In this search engine we focus on the synopsis (anime description) and firstly we create a vocabulary that maps each word we find to an integer.\n",
    "<br>\n",
    "Secondly we create an inverted index dictionary that has the following structure: \n",
    "\n",
    "Key: index_word \n",
    "\n",
    "Value: array that contains all the anime where the word is in the description of that anime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zYOICje8i5Zg",
   "metadata": {
    "id": "zYOICje8i5Zg"
   },
   "source": [
    "### 2.1.1 Create your index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLGtDUZzjPmR",
   "metadata": {
    "id": "aLGtDUZzjPmR"
   },
   "source": [
    "Function that creates the vocabulary and the inverted-index dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-8OjQQgajB3o",
   "metadata": {
    "id": "-8OjQQgajB3o"
   },
   "outputs": [],
   "source": [
    "def create_voc_ind(start=1):\n",
    "    vocabulary = {}\n",
    "    dictionary = {}\n",
    "    count = 0\n",
    "    for i in range(19130):\n",
    "        # anime 7242 and 15009 gave us a \"page_not_found\" so we skipped them while creating the tsv, so we do the same here\n",
    "        if (start+i) == 7242 or (start+i) == 15009:\n",
    "            continue\n",
    "        # open the tsv of each anime to retrieve the information\n",
    "        with open(\"File tsv/anime_\"+str(start+i)+\".tsv\", encoding=\"utf-8\") as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            l = []\n",
    "            for row in rd:\n",
    "                l.append(row)\n",
    "            text = l[2][10]  # get the synopsis of the anime\n",
    "\n",
    "        # use the function that do the preprocessing operations on the text\n",
    "        new_synopsis = pre_process_text(text)\n",
    "\n",
    "        # for each word in the description after the preprocess\n",
    "        for word in new_synopsis:\n",
    "            # if word is not in vocabulary, add it and assign it a value (incremental)\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = count\n",
    "                count += 1\n",
    "            # get the correspondent integer value in the vocabulary\n",
    "            val = vocabulary[word]\n",
    "            # if the value (word) is in the dictionary and the anime wasn't added yet, add it\n",
    "            if val in dictionary and 'anime_'+str(start+i) not in dictionary[val]:\n",
    "                dictionary[val].append('anime_'+str(start+i))\n",
    "            # if it's not in the dictionary create the key and add a list that contain the anime as a value\n",
    "            elif val not in dictionary:\n",
    "                dictionary[val] = ['anime_'+str(start+i)]\n",
    "\n",
    "    # Save vocabulary and dictionary (inverted index)\n",
    "    with open('vocabulary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocabulary, f, ensure_ascii=False, indent=4)\n",
    "    with open('dictionary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dictionary, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z3McKJU-lwbe",
   "metadata": {
    "id": "z3McKJU-lwbe"
   },
   "source": [
    "Executing this code we will have the files \"vocabulary.json\" and \"dictionary.json\" (our inverted index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0sOys9LnPtA",
   "metadata": {
    "id": "w0sOys9LnPtA"
   },
   "source": [
    "### 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dt6Y2P21n2zF",
   "metadata": {
    "id": "Dt6Y2P21n2zF"
   },
   "source": [
    "Firstly, we **load the vocabulary and dictionary** previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "QYrYnx5Tn9jE",
   "metadata": {
    "id": "QYrYnx5Tn9jE"
   },
   "outputs": [],
   "source": [
    "with open('vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "    vocabulary = json.load(f)\n",
    "with open('dictionary.json', 'r', encoding='utf-8') as f:\n",
    "    dictionary = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bxDty6u1ncH_",
   "metadata": {
    "id": "bxDty6u1ncH_"
   },
   "source": [
    "Secondly, we create the function that **gets the query and elaborates it** (preprocessing and translation from word to index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sm-_Toy2nV9E",
   "metadata": {
    "id": "sm-_Toy2nV9E"
   },
   "outputs": [],
   "source": [
    "def get_query(vocabulary):\n",
    "    # ask the user for the query\n",
    "    query = input('What are you looking for?')\n",
    "    # preprocess the query\n",
    "    query = pre_process_text(query)\n",
    "    \n",
    "    for i in range(len(query)):\n",
    "        # check if word in query is present in vocabulary\n",
    "        if query[i] not in vocabulary:\n",
    "            print(\"the word \"+query[i]+\" is not present in documents\")\n",
    "            return\n",
    "        # translate query using vocabulary (get the correspondent index)\n",
    "        else:\n",
    "            query[i] = str(vocabulary[query[i]])\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94jWJfaeoFdH",
   "metadata": {
    "id": "94jWJfaeoFdH"
   },
   "source": [
    "Also we create the function that **displays the results** we get from the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zFfDYkeJoWD_",
   "metadata": {
    "id": "zFfDYkeJoWD_"
   },
   "outputs": [],
   "source": [
    "def display_res(results):\n",
    "    print(\"Results found: \", len(results))\n",
    "    to_display=[]\n",
    "    for anime in results:\n",
    "        path = \"File tsv/\"+anime+\".tsv\"\n",
    "        \n",
    "        # access the corresponding tsv file to take out title and description\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        title = df.animeTitle.item()\n",
    "        descr = df.animeDescription.item()\n",
    "        \n",
    "        # open the file with urls to get the url of the anime\n",
    "        f = open('anime_url.txt')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        url = lines[int(anime[6:])-1]\n",
    "        \n",
    "        # put it together\n",
    "        to_display.append([title, descr, url])\n",
    "    display(pd.DataFrame(to_display, columns = ['Anime title', 'Anime description', 'Url']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RWwbbhchobVs",
   "metadata": {
    "id": "RWwbbhchobVs"
   },
   "source": [
    "Finally, we can create our effective **search engine** using the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ZjEszSXSooEO",
   "metadata": {
    "id": "ZjEszSXSooEO"
   },
   "outputs": [],
   "source": [
    "def search():\n",
    "    # getting the query and preprocessing it\n",
    "    query = get_query(vocabulary)\n",
    "    if query:\n",
    "        possible = []\n",
    "        for i in range(len(query)):\n",
    "            # find the documents where the word (index) is, using the inverted index\n",
    "            possible.append(set(dictionary[str(query[i])]))\n",
    "        # get the documents where there are all the words in the query doing the intersection\n",
    "        result = set.intersection(*possible)\n",
    "        if result:\n",
    "        # return information for the results\n",
    "           display_res(result)\n",
    "        else:\n",
    "            print('No matches found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XDa6YoXVo9Dz",
   "metadata": {
    "id": "XDa6YoXVo9Dz"
   },
   "source": [
    "Now we can test it using the query suggested \"saiyan race\" and see what our search engine returns as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b857de95-68bb-42cc-835e-f62c3feffb53",
   "metadata": {
    "id": "b857de95-68bb-42cc-835e-f62c3feffb53",
    "outputId": "5996b408-7649-4844-ccc4-421c0921aab1"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What are you looking for? saiyan race\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results found:  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anime title</th>\n",
       "      <th>Anime description</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Anime title  \\\n",
       "0                                    Dragon Ball Kai   \n",
       "1                                      Dragon Ball Z   \n",
       "2  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "3                           Dragon Ball Super: Broly   \n",
       "\n",
       "                                   Anime description  \\\n",
       "0  Five years after the events of Dragon Ball, ma...   \n",
       "1  Five years after winning the World Martial Art...   \n",
       "2  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "3  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://myanimelist.net/anime/6033/Dragon_Ball...  \n",
       "1  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n  \n",
       "2  https://myanimelist.net/anime/986/Dragon_Ball_...  \n",
       "3  https://myanimelist.net/anime/36946/Dragon_Bal...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KMb-NBDcpTnO",
   "metadata": {
    "id": "KMb-NBDcpTnO"
   },
   "source": [
    "As we can see we obtain 4 results, and it give us, for each result: \n",
    "\n",
    "\n",
    "*   Title\n",
    "*   Description\n",
    "*   Url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fnstGPPer7yj",
   "metadata": {
    "id": "fnstGPPer7yj"
   },
   "source": [
    "# 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YcnVHZw5sVwI",
   "metadata": {
    "id": "YcnVHZw5sVwI"
   },
   "source": [
    "In this search engine we still focus on the synopsis (anime description) but creating a ranking score using the tfidf.\n",
    "<br>\n",
    "We use the vocabulary previously created but this time we will build an inverted-index having the following structure:\n",
    "\n",
    "Key: index_word\n",
    "\n",
    "Value: array that contains all the anime where the word is in the description of that anime and the correspondent tfidf score for each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Vqi8m12m30V",
   "metadata": {
    "id": "1Vqi8m12m30V"
   },
   "source": [
    "### 2.2.1 Create your index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FfLX4mAlnGlf",
   "metadata": {
    "id": "FfLX4mAlnGlf"
   },
   "source": [
    "Function that creates the inverted index dictionary and computes the tfidf in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AMx2aXR7s0nJ",
   "metadata": {
    "id": "AMx2aXR7s0nJ"
   },
   "outputs": [],
   "source": [
    "def create_inv_ind_tfidf(vocabulary, dictionary, start = 1):\n",
    "    n_anime = 19130\n",
    "    dictionary_tfidf = {}\n",
    "    for i in range(n_anime):\n",
    "        # skipping broken pages\n",
    "        if (start+i) == 7242 or (start+i) == 15009:\n",
    "            continue\n",
    "            \n",
    "        with open(\"File tsv/anime_\"+str(start+i)+\".tsv\", encoding=\"utf-8\") as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            l = []\n",
    "            for row in rd:\n",
    "                l.append(row)\n",
    "            text = l[2][10]  # the synopsis of the anime\n",
    "            # use the function that do the preprocessing operations on the synopsis\n",
    "            text = pre_process_text(text)\n",
    "\n",
    "        word_text = len(text)\n",
    "        \n",
    "        # TF\n",
    "        for word in text:\n",
    "            # get the id of the word\n",
    "            word_id = str(vocabulary[word])\n",
    "            if word_id in dictionary_tfidf:\n",
    "                if dictionary_tfidf[word_id][-1][0] != 'anime_'+str(start+i):\n",
    "                    dictionary_tfidf[word_id].append(\n",
    "                        ['anime_'+str(ini+i), 1, word_text])\n",
    "                else:\n",
    "                    dictionary_tfidf[word_id][-1][1] += 1\n",
    "            # if it's not in the dictionary create the key and add a list that contain the anime as a value\n",
    "            else:\n",
    "                dictionary_tfidf[word_id] = [['anime_'+str(start+i), 1, word_text]]\n",
    "\n",
    "    # For each word, compute the tfidf related to each document where it appears\n",
    "    inv_indx_tfidf = {}\n",
    "    for word in dictionary_tfidf:\n",
    "        inv_indx_tfidf[word] = []\n",
    "        for doc in dictionary_tfidf[word]:\n",
    "            inv_indx_tfidf[word].append((\n",
    "                doc[0], doc[1]/doc[2]*np.log10(n_anime/len(dictionary[word]))))\n",
    "\n",
    "    with open('inv_ind_TFIDF.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(inv_indx_tfidf, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sA2t1UP9oPec",
   "metadata": {
    "id": "sA2t1UP9oPec"
   },
   "source": [
    "Executing this code we will have the file \"inv_ind_TFIDF.json\" that contains the inverted index with tfidf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YJOmZHGDocpt",
   "metadata": {
    "id": "YJOmZHGDocpt"
   },
   "source": [
    "### 2.2.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e70eb0-bfa3-46d4-ab2d-bb5dcf5af18d",
   "metadata": {},
   "source": [
    "As before, we need to load the index we have just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43680b76-d84a-4488-84e3-346a732b6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inv_ind_TFIDF.json', 'r', encoding='utf-8') as f:\n",
    "    inv_ind_tfidf = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AoxS8arGpbrS",
   "metadata": {
    "id": "AoxS8arGpbrS"
   },
   "source": [
    "Firstly we need the function that **calculates the norm of a document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mM25jrv4pIDx",
   "metadata": {
    "id": "mM25jrv4pIDx"
   },
   "outputs": [],
   "source": [
    "def norm_doc(docname, inv_ind, vocabulary):\n",
    "    doc_vector = []\n",
    "    # accessing the anime description\n",
    "    with open('File tsv/'+docname+'.tsv', encoding = 'utf-8') as f:\n",
    "        rd = csv.reader(f, delimiter=\"\\t\", quotechar='\"')\n",
    "        l=[]\n",
    "        for row in rd:\n",
    "            l.append(row) \n",
    "        text=l[2][10]\n",
    "    new_text = pre_process_text(text)\n",
    "    new_text = [str(vocabulary[word]) for word in new_text]\n",
    "    # extracting the tf-idfs of all the words in the document\n",
    "    for w in new_text:\n",
    "        for el in inv_ind[w]:\n",
    "            if el[0] == docname:\n",
    "                doc_vector.append(el[1])\n",
    "    doc_vector = np.array(doc_vector)\n",
    "    return np.linalg.norm(doc_vector,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y01IFLOwqK2n",
   "metadata": {
    "id": "y01IFLOwqK2n"
   },
   "source": [
    "Then we need to **create a vector for the query** that we can use later to compute the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gVYKY3nspMIf",
   "metadata": {
    "id": "gVYKY3nspMIf"
   },
   "outputs": [],
   "source": [
    "def vectorize_query(query, inv_ind, vocabulary):\n",
    "    q_vector = []\n",
    "    for word in query:\n",
    "        idf = np.log(19130/len(inv_ind[word]))\n",
    "        q_vector.append(idf)\n",
    "    return np.array(q_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kDeWGlPtqcal",
   "metadata": {
    "id": "kDeWGlPtqcal"
   },
   "source": [
    "We also need a function that **computes the cosine_similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "F9UapttKpRaS",
   "metadata": {
    "id": "F9UapttKpRaS"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(docname, query, inv_ind, vocabulary):\n",
    "    # compute the norm of the vectorized document\n",
    "    norm_d = norm_doc(docname, inv_ind, vocabulary)\n",
    "    # vectorize the query\n",
    "    vector_q = vectorize_query(query, inv_ind, vocabulary)\n",
    "    vector_d = []\n",
    "    # vectorize the document according to the query\n",
    "    for w in query:\n",
    "        for el in inv_ind[w]:\n",
    "            if el[0] == docname:\n",
    "                vector_d.append(el[1])\n",
    "    vector_d = np.array(vector_d)\n",
    "    # compute the norm of the vectorized query\n",
    "    norm_q = np.linalg.norm(vector_q)\n",
    "    # compute the cosine similarity\n",
    "    return np.dot(vector_q,vector_d)/(norm_d*norm_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TL7DwSoCq4g_",
   "metadata": {
    "id": "TL7DwSoCq4g_"
   },
   "source": [
    "Finally, also in this case, we create a function that **displays the results** obtained from the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4XQV2uQApWtu",
   "metadata": {
    "id": "4XQV2uQApWtu"
   },
   "outputs": [],
   "source": [
    "def display_res_score(results):\n",
    "    print(\"Top\", len(results), 'documents')\n",
    "    to_display=[]\n",
    "    for score, anime in results:\n",
    "        # accessing the .tsv file corresponding to the anime\n",
    "        path = \"File tsv/\"+anime+\".tsv\"\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        # retrieving anime title and description\n",
    "        title = df.animeTitle.item()\n",
    "        descr = df.animeDescription.item()\n",
    "        # retrieving the url of the anime\n",
    "        f = open('anime_url.txt')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        url = lines[int(anime[6:])-1]\n",
    "        # putting it all together\n",
    "        to_display.append([title, descr, url, round(score, 2)])\n",
    "    display(pd.DataFrame(to_display, columns = ['Anime title', 'Anime description', 'Url', 'Score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZiOLq8pTrWX9",
   "metadata": {
    "id": "ZiOLq8pTrWX9"
   },
   "source": [
    "Now we can create our **search engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "KnPgwZbjrbq-",
   "metadata": {
    "id": "KnPgwZbjrbq-"
   },
   "outputs": [],
   "source": [
    "def search2():\n",
    "    query= get_query(vocabulary)\n",
    "    if query:\n",
    "        possible = []\n",
    "        for i in range(len(query)):\n",
    "            # find the documents where the word (index) is, using the inverted index\n",
    "            possible.append(set(dictionary[query[i]]))\n",
    "        # get the documents where there are all the words in the query doing the intersection\n",
    "        result = set.intersection(*possible)\n",
    "        if result:\n",
    "        # initialize the heap for storing scores\n",
    "            heap = list()\n",
    "            heapq.heapify(heap)\n",
    "            # compute cosine similarity for all matching documents\n",
    "            for r in result:\n",
    "                similarity = cosine_similarity(r, query, inv_ind_tfidf, vocabulary)\n",
    "                heapq.heappush(heap, (similarity, r))\n",
    "            # retrieve top 5 documents from the heap\n",
    "            top_5 = heapq.nlargest(5, heap)\n",
    "            display_res_score(top_5)\n",
    "        else:\n",
    "            print('No matches found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gHAHoSpIrpmw",
   "metadata": {
    "id": "gHAHoSpIrpmw"
   },
   "source": [
    "As we did before, we **try it out** using the same query \"saiyan race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af35098-6479-4f49-9d4f-1190ab488c5d",
   "metadata": {
    "id": "8af35098-6479-4f49-9d4f-1190ab488c5d",
    "outputId": "98245dfc-7a91-444e-ea1e-2bd26400347c"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What are you looking for? saiyan race\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anime title</th>\n",
       "      <th>Anime description</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Anime title  \\\n",
       "0  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1                           Dragon Ball Super: Broly   \n",
       "2                                      Dragon Ball Z   \n",
       "3                                    Dragon Ball Kai   \n",
       "\n",
       "                                   Anime description  \\\n",
       "0  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "2  Five years after winning the World Martial Art...   \n",
       "3  Five years after the events of Dragon Ball, ma...   \n",
       "\n",
       "                                                 Url  Score  \n",
       "0  https://myanimelist.net/anime/986/Dragon_Ball_...   0.31  \n",
       "1  https://myanimelist.net/anime/36946/Dragon_Bal...   0.10  \n",
       "2  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n   0.07  \n",
       "3  https://myanimelist.net/anime/6033/Dragon_Ball...   0.06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GpKVL5ihsBG2",
   "metadata": {
    "id": "GpKVL5ihsBG2"
   },
   "source": [
    "As we can see, we have as a results the same anime obtained before but ordered by the similarity score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4531098-f811-41cc-91f9-58cd59c67416",
   "metadata": {},
   "source": [
    "# 3 - Define a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48b2c9-6725-4d28-b031-cd02fadaa986",
   "metadata": {},
   "source": [
    "We are going to use the cosine similarity we have defined in the previous exercise as a starting point for our new score. At the same time, now we will also take into consideration:\n",
    "- the <b>title</b> of the anime: each of the matched documents will receive a bonus proportional to the number of words in the query that can also be found in the title $\\rightarrow$ <b>'boosttitle'</b> function\n",
    "- the <b>ranking</b> and the <b>popularity</b>: the more popular and appreciated the anime, the more likely it is that a user could be looking for it, so we are going to give the matched documents a boost based on their performance in rankings and popularity $\\rightarrow$ <b>'boostrank'</b> and <b>'boostpopularity'</b> functions\n",
    "- the <b>type</b> of the anime: we are going to give the possibility to the user to input an additional query specifying the type of anime they are looking for. This makes sense if we think about the fact that many animes have different versions/remakes, and one could be looking for a specific version of their favorite anime $\\rightarrow$ <b>'matchtype'</b> function \n",
    "- the <b>date</b> the anime was released: just like before, the user will be able to specify a range of dates (in the format YYYY-YYYY) that roughly correspond to the time the anime he's looking for was released $\\rightarrow$ <b>'boostdate'</b> function\n",
    "<br><br>\n",
    "<b>Remark n.1</b>: as specified in the assignment, the additional queries <b>do not</b> filter the results. Instead, if they are present, results that match them will simply receive a bonus in their final score.\n",
    "<br><br>\n",
    "The <b>'boostscore'</b> function creates a new score that will look like this:\n",
    "<br><br>\n",
    "<div align = center>$\\frac{cosine\\ similarity + potential\\ bonuses}{3.4}$</div>\n",
    "<br>\n",
    "where 3.4 is the maximum possible score (1 for the cosine similarity and 2.4 for our bonuses).\n",
    "<br><br>\n",
    "<b>Remark n.2</b>: dividing for the maximum score does not only allow us to make direct confrontations with the raw cosine similarities used for Exercise 2, but is also a way of 'penalising' results that perform poorly with respect to the additional bonuses. In this way, the scoring function is actually more balanced because it does not only boost scores but it can also bring them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90439ead-e264-4feb-b8cb-c0afd764b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosttitle(query, title):\n",
    "    count = 0\n",
    "    # removing punctuation and capital letters\n",
    "    title = title.lower()\n",
    "    title = title.translate(str.maketrans('', '', string.punctuation))\n",
    "    title = title.split()\n",
    "    # translating title \n",
    "    for i in range(len(title)):\n",
    "        if title[i] in vocabulary:\n",
    "            title[i] = str(vocabulary[title[i]])\n",
    "    # counting matches and computing bonus (maximum value is 1)       \n",
    "    for w in query:\n",
    "        if w in title:\n",
    "            count += 1\n",
    "    res = count/len(title)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fcf2af6-ff58-4b4b-a10d-18015ee5bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostpopularity(pop):\n",
    "    res = 0\n",
    "    if pop < 50:\n",
    "        res += 0.4\n",
    "    elif pop < 100:\n",
    "        res += 0.3\n",
    "    elif pop < 1000:\n",
    "        res += 0.2\n",
    "    elif pop < 5000:\n",
    "        res += 0.1\n",
    "    elif pop < 10000:\n",
    "        res += 0.05\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d1f9a0-6081-435d-9fba-9f65329ef0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrank(rank):\n",
    "    res = 0\n",
    "    if rank < 50:\n",
    "        res += 0.4\n",
    "    elif rank < 100:\n",
    "        res += 0.3\n",
    "    elif rank < 1000:\n",
    "        res += 0.2\n",
    "    elif rank < 5000:\n",
    "        res += 0.1\n",
    "    elif rank < 10000:\n",
    "        res += 0.05\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a62af493-967f-4346-8799-775971f3d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchtype(animetype, typequery):\n",
    "    res = 0\n",
    "    if typequery:\n",
    "        # checking if the type of the anime matches with the specified query\n",
    "        if typequery == animetype:\n",
    "            res += 0.4\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f76f84f2-03da-4388-823b-3205f9f47ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_in_range(start, end, x):\n",
    "    # checking if date x is in the specified range or not\n",
    "    if start <= end:\n",
    "        return start <= x <= end\n",
    "    else:\n",
    "        return start <= x or x <= end\n",
    "    \n",
    "def boostdate(animedate, datequery):\n",
    "    res = 0\n",
    "    if datequery:\n",
    "        # convert to datetime\n",
    "        datequery = datequery.split('-')\n",
    "        start = datetime(int(datequery[0]),1,1)\n",
    "        end = datetime(int(datequery[1]),1,1)\n",
    "        if animedate:\n",
    "            try:\n",
    "                year = datetime.strptime(animedate, '%Y-%m-%d').year\n",
    "                year = datetime(year, 1,1)\n",
    "            except:\n",
    "                year = datetime(animedate,1,1)\n",
    "            # checking and adding bonus\n",
    "            if time_in_range(start,end,year):\n",
    "                res += 0.2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f17b26-e442-49bb-8c2a-81d52bbd929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostscore(anime, cosine_similarity, query, typequery, datequery):\n",
    "    # accessing  and extracting information about the anime\n",
    "    with open('File tsv/'+str(anime)+'.tsv', encoding = 'utf-8') as f:\n",
    "        df = pd.read_csv(f, delimiter='\\t')\n",
    "    title = df.animeTitle.item()\n",
    "    rank = df.animeRank.item()\n",
    "    animetype = df.animeType.item()\n",
    "    animedate = df.releaseDate.item()\n",
    "    popularity = df.animePopularity.item()\n",
    "    # computing all the bonuses for the anime\n",
    "    bonuses = boosttitle(query,title) + boostrank(rank) + matchtype(animetype, typequery) + boostdate(animedate, datequery) + boostpopularity(popularity)\n",
    "    # computing final score after bonuses\n",
    "    return (bonuses + cosine_similarity)/3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd82bf6e-73e1-47d9-a0f6-1ad356b9444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_query(vocabulary):\n",
    "    # ask the user for the main and the additional queries\n",
    "    query = input('What are you looking for?')\n",
    "    datequery = input('From which period is it from?')\n",
    "    typequery = input('What type of anime is it?')\n",
    "    # preprocess the query\n",
    "    query = pre_process_text(query)\n",
    "    \n",
    "    for i in range(len(query)):\n",
    "        # check if query matches\n",
    "        if query[i] not in vocabulary:\n",
    "            print('No matches found')\n",
    "            return\n",
    "        # translate query using vocabulary\n",
    "        else:\n",
    "            query[i] = str(vocabulary[query[i]])\n",
    "    return query, datequery, typequery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d03aa6-9157-4dcf-ab9a-95a5d873244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search3():\n",
    "    query, datequery, typequery = get_new_query(vocabulary)\n",
    "    if query:\n",
    "        possible = []\n",
    "        for i in range(len(query)):\n",
    "            # find the documents where the word (index) is, using the inverted index\n",
    "            possible.append(set(dictionary[query[i]]))\n",
    "        # get the documents where there are all the words in the query doing the intersection\n",
    "        result = set.intersection(*possible)\n",
    "        if result:\n",
    "        # initialize the heap for storing scores\n",
    "            heap = list()\n",
    "            heapq.heapify(heap)\n",
    "            # compute the new score for all matching documents\n",
    "            for r in result:\n",
    "                similarity = cosine_similarity(r, query, inv_ind_tfidf, vocabulary)\n",
    "                new_score = boostscore(r, similarity, query, typequery, datequery)\n",
    "                heapq.heappush(heap, (new_score, r))\n",
    "            # retrieve top 5 documents from the heap\n",
    "            top_5 = heapq.nlargest(5, heap)\n",
    "            display_res_score(top_5)\n",
    "        else:\n",
    "            print('No matches found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90342116-3285-472a-a365-2d6311e7d29c",
   "metadata": {},
   "source": [
    "Let's try our new score out with our usual query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3745fa9-cce9-4543-9f68-4daf6bdfbc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What are you looking for? saiyan race\n",
      "From which period is it from? 1985-1995\n",
      "What type of anime is it? TV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anime title</th>\n",
       "      <th>Anime description</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Anime title  \\\n",
       "0                                      Dragon Ball Z   \n",
       "1                                    Dragon Ball Kai   \n",
       "2  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "3                           Dragon Ball Super: Broly   \n",
       "\n",
       "                                   Anime description  \\\n",
       "0  Five years after winning the World Martial Art...   \n",
       "1  Five years after the events of Dragon Ball, ma...   \n",
       "2  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "3  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "\n",
       "                                                 Url  Score  \n",
       "0  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n   0.34  \n",
       "1  https://myanimelist.net/anime/6033/Dragon_Ball...   0.22  \n",
       "2  https://myanimelist.net/anime/986/Dragon_Ball_...   0.21  \n",
       "3  https://myanimelist.net/anime/36946/Dragon_Bal...   0.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b91349-4f25-41cf-b77a-a12154501900",
   "metadata": {},
   "source": [
    "As we see, results our now more accurate with respect to the user query: the best result here is a more popular anime that matches both the type and the range of dates that was specified. We can see that the previous best result is now in third place, with a lower score: as we mentioned before, it must have gotten penalised by poor performance in terms of popularity, rank and type mismatch with respect to the query. At the same time, the second result matches the type specified in the query but not the dates. This is a matter of how bonuses are actually weighted: since dates are a bit more hard to remember correctly, we think it makes sense in this particular case to give more importance to the type of the anime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r6mzmj10sd73",
   "metadata": {
    "id": "r6mzmj10sd73"
   },
   "source": [
    "\n",
    "# 5 - Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scDaPtpMsr2d",
   "metadata": {
    "id": "scDaPtpMsr2d"
   },
   "source": [
    "### 1) Write an algorithm that computes the acceptable solution with the longest possible duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho7MSvY2tDwk",
   "metadata": {
    "id": "Ho7MSvY2tDwk"
   },
   "source": [
    "This function computes the longest possible duration and call the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GUTxWtSlskN8",
   "metadata": {
    "id": "GUTxWtSlskN8"
   },
   "outputs": [],
   "source": [
    "# Function that finds the longest duration of appointment respecting the constraint\n",
    "def best_plan_appointment(x):\n",
    "    # create a new list of the same dimension setting the first and second elements\n",
    "    t = [0 for _ in range(len(x))]\n",
    "    t[0] = x[0]\n",
    "    t[1] = x[1]\n",
    "    for i in range(2, len(x)):\n",
    "        # find at each position what is the maximum value we can get respecting the constraint\n",
    "        t[i] = max(x[i], max(t[:i-1])+x[i])\n",
    "\n",
    "    return max(t), find_values_subseq(x, t, max(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aCf9WaL9s9f2",
   "metadata": {
    "id": "aCf9WaL9s9f2"
   },
   "source": [
    "### 2) Implement a program that given in input an instance in the form given above, gives the optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BvXOdf3JthyD",
   "metadata": {
    "id": "BvXOdf3JthyD"
   },
   "source": [
    "This function receives in input what we obtained before and find out what are the appointment to choose to get the best one found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DBtJ0keps-x0",
   "metadata": {
    "id": "DBtJ0keps-x0"
   },
   "outputs": [],
   "source": [
    "# Function used to find the value that give us the best solution\n",
    "def find_values_subseq(x, lista, best):\n",
    "    # both the lists are reversed\n",
    "    lista.reverse()\n",
    "    x.reverse()\n",
    "    ris = []\n",
    "    #when i find that value i subtract it and i know that this is one of the values of the solution\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == best:\n",
    "            best -= x[i]\n",
    "            ris.append(x[i])\n",
    "    ris.reverse()\n",
    "    return ris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CtM71or5ueFD",
   "metadata": {
    "id": "CtM71or5ueFD"
   },
   "source": [
    "We can try them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qICJ2dYYufsa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1636830954328,
     "user": {
      "displayName": "Alex Onofri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12251553323385076301"
     },
     "user_tz": -60
    },
    "id": "qICJ2dYYufsa",
    "outputId": "ac0e5263-34cb-46da-f3e6-66f4ca63ba41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, [40, 50, 20])\n"
     ]
    }
   ],
   "source": [
    "print(best_plan_appointment([30, 40, 25, 50, 30, 20]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
